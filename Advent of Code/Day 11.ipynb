{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = open(\"Day 11 test input.txt\", \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "for line in test:\n",
    "    line = line.strip(\"\\n\")\n",
    "    test_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"Day 11 input.txt\", \"r\")\n",
    "real_data = []\n",
    "for line in f:\n",
    "    line = line.strip(\"\\n\")\n",
    "    real_data.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['...#......',\n",
       " '.......#..',\n",
       " '#.........',\n",
       " '..........',\n",
       " '......#...',\n",
       " '.#........',\n",
       " '.........#',\n",
       " '..........',\n",
       " '.......#..',\n",
       " '#...#.....']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find all hashtags and stores their locations (returns 2 by x matrix)\n",
    "# function to handle looping through all hashtags (equal to # of pairs)\n",
    "# function to calculate the distance between two hashtags\n",
    "def calc_distance(hash_1, hash_2):\n",
    "    horizontal = abs(hash_2[1] - hash_1[1])\n",
    "    vertical = abs(hash_2[0] - hash_1[0])\n",
    "    return (horizontal + vertical)\n",
    "def find_all(test_data):\n",
    "    locations = []\n",
    "    for i, row in enumerate(test_data):\n",
    "        for j, element in enumerate(row):\n",
    "            if element == '#':\n",
    "                locations.append([i, j])\n",
    "    return locations\n",
    "def handle_pairs(locations):\n",
    "    distance = 0\n",
    "    pairs = 0\n",
    "    # for hash in locations:\n",
    "    #     other_hashes = [other_hash for other_hash in locations if other_hash != hash]\n",
    "    #     for other_hash in other_hashes:\n",
    "    #         distance += calc_distance(hash, other_hash)\n",
    "    #         pairs += 1\n",
    "    #     locations.remove(hash)\n",
    "    #     # print(locations)\n",
    "    #     # print(pairs)\n",
    "    # for hash in locations: # There is a glitch in the matrix! for some reason the prev. loop stops when 4 elements remain, so I continue it here lol\n",
    "    #     other_hashes = [other_hash for other_hash in locations if other_hash != hash]\n",
    "    #     for other_hash in other_hashes:\n",
    "    #         distance += calc_distance(hash, other_hash)            \n",
    "    #         pairs += 1\n",
    "    #     locations.remove(hash)\n",
    "    #     # print(locations)\n",
    "    #     # print(pairs)\n",
    "    # for hash in locations: # HAHA! same thing when only 1 pair is remaining! WTH!\n",
    "    #     other_hashes = [other_hash for other_hash in locations if other_hash != hash]\n",
    "    #     for other_hash in other_hashes:\n",
    "    #         distance += calc_distance(hash, other_hash)       \n",
    "    #         pairs += 1\n",
    "    #     locations.remove(hash)\n",
    "    # print(len(locations))\n",
    "    # for hash in locations: # again for real data.. let's see\n",
    "    #     other_hashes = [other_hash for other_hash in locations if other_hash != hash]\n",
    "    #     for other_hash in other_hashes:\n",
    "    #         distance += calc_distance(hash, other_hash)       \n",
    "    #         pairs += 1\n",
    "    #     locations.remove(hash)\n",
    "    # print(len(locations))\n",
    "    # for hash in locations: # again for real data.. let's see\n",
    "    #     other_hashes = [other_hash for other_hash in locations if other_hash != hash]\n",
    "    #     for other_hash in other_hashes:\n",
    "    #         distance += calc_distance(hash, other_hash)       \n",
    "    #         pairs += 1\n",
    "    #     locations.remove(hash)\n",
    "    # print(len(locations))\n",
    "    # for hash in locations: # again for real data.. let's see\n",
    "    #     other_hashes = [other_hash for other_hash in locations if other_hash != hash]\n",
    "    #     for other_hash in other_hashes:\n",
    "    #         distance += calc_distance(hash, other_hash)       \n",
    "    #         pairs += 1\n",
    "    #     locations.remove(hash)\n",
    "    # print(len(locations))\n",
    "    # for hash in locations: # again for real data.. let's see\n",
    "    #     other_hashes = [other_hash for other_hash in locations if other_hash != hash]\n",
    "    #     for other_hash in other_hashes:\n",
    "    #         distance += calc_distance(hash, other_hash)       \n",
    "    #         pairs += 1\n",
    "    #     locations.remove(hash)\n",
    "    # print(len(locations))\n",
    "    # for hash in locations: # again for real data.. let's see\n",
    "    #     other_hashes = [other_hash for other_hash in locations if other_hash != hash]\n",
    "    #     for other_hash in other_hashes:\n",
    "    #         distance += calc_distance(hash, other_hash)       \n",
    "    #         pairs += 1\n",
    "    #     locations.remove(hash)\n",
    "    # print(len(locations))\n",
    "    while locations:\n",
    "        hash = locations.pop(0)  # Remove the first element from locations\n",
    "        for other_hash in locations:\n",
    "            distance += calc_distance(hash, other_hash)\n",
    "            pairs += 1\n",
    "        print(pairs)\n",
    "    return distance\n",
    "# now expanding the universe function:\n",
    "def expand_rows(data):\n",
    "    expanding_rows = data.copy()\n",
    "    insertions = 0\n",
    "    for i in range(len(data)):\n",
    "        if all(element == '.' for element in data[i]):\n",
    "            expanding_rows.insert(i+insertions,data[i])\n",
    "            insertions += 1\n",
    "    return expanding_rows\n",
    "def expand_columns(data): # figured out an easier way! just transpose the data and use expand_rows()!\n",
    "    expanding_columns = data.copy()\n",
    "    insertions = 0\n",
    "    for j in range(len(data[0])):\n",
    "        column = []\n",
    "        column.append([data[x][j] for x in range(len(data))]) # super inefficient.. dataframe would be better\n",
    "        if all(element == '.' for element in column[0]): # used [0] cuz it's a list of list somehow\n",
    "            #expanding_columns.insert() # super inefficient cuz gotta manually insert each element!\n",
    "            for row in range(len(data)):\n",
    "                expanding_columns[row] = expanding_columns[row][:j+insertions] + column[0][row] + expanding_columns[row][j+insertions:]\n",
    "            insertions += 1\n",
    "    return expanding_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "hi\n",
      "hi\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['....#........',\n",
       " '.........#...',\n",
       " '#............',\n",
       " '.............',\n",
       " '........#....',\n",
       " '.#...........',\n",
       " '............#',\n",
       " '.............',\n",
       " '.........#...',\n",
       " '#....#.......']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_columns(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['...#......',\n",
       " '.......#..',\n",
       " '#.........',\n",
       " '..........',\n",
       " '..........',\n",
       " '......#...',\n",
       " '.#........',\n",
       " '.........#',\n",
       " '..........',\n",
       " '..........',\n",
       " '.......#..',\n",
       " '#...#.....']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_rows(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_pairs(find_all(expand_columns(expand_rows(test_data)))) # it works when 3 loops are there lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(find_all(expand_columns(expand_rows(real_data)))) # correct! it gets the exact number of #s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9464014"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = handle_pairs(find_all(expand_columns(expand_rows(real_data))))\n",
    "answer # Hey GPT-4, here is the wrong answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9570271"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_pairs(find_all(expand_columns(expand_rows(real_data)))) # it increased after adding the for loop again lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9603576"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_pairs(find_all(expand_columns(expand_rows(real_data)))) # it increased after adding 2 extra loops!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9604793"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_pairs(find_all(expand_columns(expand_rows(real_data)))) # added extra loop and printed how many #s remaining, only 3, good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54\n",
      "27\n",
      "13\n",
      "6\n",
      "3\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9605127"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_pairs(find_all(expand_columns(expand_rows(real_data)))) # I think since only 1 is remaining at the end, I reached the correct answer? let's see!\n",
    "# CORRECT!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The error was due to:\n",
    "# The issue you encountered in your handle_pairs() function relates to a common pitfall in Python: modifying a list while \n",
    "# iterating over it. When you remove elements from locations within the loop (for hash in locations:), it disrupts the loop's \n",
    "# internal index tracking. As a result, the loop skips over elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "437\n",
      "873\n",
      "1308\n",
      "1742\n",
      "2175\n",
      "2607\n",
      "3038\n",
      "3468\n",
      "3897\n",
      "4325\n",
      "4752\n",
      "5178\n",
      "5603\n",
      "6027\n",
      "6450\n",
      "6872\n",
      "7293\n",
      "7713\n",
      "8132\n",
      "8550\n",
      "8967\n",
      "9383\n",
      "9798\n",
      "10212\n",
      "10625\n",
      "11037\n",
      "11448\n",
      "11858\n",
      "12267\n",
      "12675\n",
      "13082\n",
      "13488\n",
      "13893\n",
      "14297\n",
      "14700\n",
      "15102\n",
      "15503\n",
      "15903\n",
      "16302\n",
      "16700\n",
      "17097\n",
      "17493\n",
      "17888\n",
      "18282\n",
      "18675\n",
      "19067\n",
      "19458\n",
      "19848\n",
      "20237\n",
      "20625\n",
      "21012\n",
      "21398\n",
      "21783\n",
      "22167\n",
      "22550\n",
      "22932\n",
      "23313\n",
      "23693\n",
      "24072\n",
      "24450\n",
      "24827\n",
      "25203\n",
      "25578\n",
      "25952\n",
      "26325\n",
      "26697\n",
      "27068\n",
      "27438\n",
      "27807\n",
      "28175\n",
      "28542\n",
      "28908\n",
      "29273\n",
      "29637\n",
      "30000\n",
      "30362\n",
      "30723\n",
      "31083\n",
      "31442\n",
      "31800\n",
      "32157\n",
      "32513\n",
      "32868\n",
      "33222\n",
      "33575\n",
      "33927\n",
      "34278\n",
      "34628\n",
      "34977\n",
      "35325\n",
      "35672\n",
      "36018\n",
      "36363\n",
      "36707\n",
      "37050\n",
      "37392\n",
      "37733\n",
      "38073\n",
      "38412\n",
      "38750\n",
      "39087\n",
      "39423\n",
      "39758\n",
      "40092\n",
      "40425\n",
      "40757\n",
      "41088\n",
      "41418\n",
      "41747\n",
      "42075\n",
      "42402\n",
      "42728\n",
      "43053\n",
      "43377\n",
      "43700\n",
      "44022\n",
      "44343\n",
      "44663\n",
      "44982\n",
      "45300\n",
      "45617\n",
      "45933\n",
      "46248\n",
      "46562\n",
      "46875\n",
      "47187\n",
      "47498\n",
      "47808\n",
      "48117\n",
      "48425\n",
      "48732\n",
      "49038\n",
      "49343\n",
      "49647\n",
      "49950\n",
      "50252\n",
      "50553\n",
      "50853\n",
      "51152\n",
      "51450\n",
      "51747\n",
      "52043\n",
      "52338\n",
      "52632\n",
      "52925\n",
      "53217\n",
      "53508\n",
      "53798\n",
      "54087\n",
      "54375\n",
      "54662\n",
      "54948\n",
      "55233\n",
      "55517\n",
      "55800\n",
      "56082\n",
      "56363\n",
      "56643\n",
      "56922\n",
      "57200\n",
      "57477\n",
      "57753\n",
      "58028\n",
      "58302\n",
      "58575\n",
      "58847\n",
      "59118\n",
      "59388\n",
      "59657\n",
      "59925\n",
      "60192\n",
      "60458\n",
      "60723\n",
      "60987\n",
      "61250\n",
      "61512\n",
      "61773\n",
      "62033\n",
      "62292\n",
      "62550\n",
      "62807\n",
      "63063\n",
      "63318\n",
      "63572\n",
      "63825\n",
      "64077\n",
      "64328\n",
      "64578\n",
      "64827\n",
      "65075\n",
      "65322\n",
      "65568\n",
      "65813\n",
      "66057\n",
      "66300\n",
      "66542\n",
      "66783\n",
      "67023\n",
      "67262\n",
      "67500\n",
      "67737\n",
      "67973\n",
      "68208\n",
      "68442\n",
      "68675\n",
      "68907\n",
      "69138\n",
      "69368\n",
      "69597\n",
      "69825\n",
      "70052\n",
      "70278\n",
      "70503\n",
      "70727\n",
      "70950\n",
      "71172\n",
      "71393\n",
      "71613\n",
      "71832\n",
      "72050\n",
      "72267\n",
      "72483\n",
      "72698\n",
      "72912\n",
      "73125\n",
      "73337\n",
      "73548\n",
      "73758\n",
      "73967\n",
      "74175\n",
      "74382\n",
      "74588\n",
      "74793\n",
      "74997\n",
      "75200\n",
      "75402\n",
      "75603\n",
      "75803\n",
      "76002\n",
      "76200\n",
      "76397\n",
      "76593\n",
      "76788\n",
      "76982\n",
      "77175\n",
      "77367\n",
      "77558\n",
      "77748\n",
      "77937\n",
      "78125\n",
      "78312\n",
      "78498\n",
      "78683\n",
      "78867\n",
      "79050\n",
      "79232\n",
      "79413\n",
      "79593\n",
      "79772\n",
      "79950\n",
      "80127\n",
      "80303\n",
      "80478\n",
      "80652\n",
      "80825\n",
      "80997\n",
      "81168\n",
      "81338\n",
      "81507\n",
      "81675\n",
      "81842\n",
      "82008\n",
      "82173\n",
      "82337\n",
      "82500\n",
      "82662\n",
      "82823\n",
      "82983\n",
      "83142\n",
      "83300\n",
      "83457\n",
      "83613\n",
      "83768\n",
      "83922\n",
      "84075\n",
      "84227\n",
      "84378\n",
      "84528\n",
      "84677\n",
      "84825\n",
      "84972\n",
      "85118\n",
      "85263\n",
      "85407\n",
      "85550\n",
      "85692\n",
      "85833\n",
      "85973\n",
      "86112\n",
      "86250\n",
      "86387\n",
      "86523\n",
      "86658\n",
      "86792\n",
      "86925\n",
      "87057\n",
      "87188\n",
      "87318\n",
      "87447\n",
      "87575\n",
      "87702\n",
      "87828\n",
      "87953\n",
      "88077\n",
      "88200\n",
      "88322\n",
      "88443\n",
      "88563\n",
      "88682\n",
      "88800\n",
      "88917\n",
      "89033\n",
      "89148\n",
      "89262\n",
      "89375\n",
      "89487\n",
      "89598\n",
      "89708\n",
      "89817\n",
      "89925\n",
      "90032\n",
      "90138\n",
      "90243\n",
      "90347\n",
      "90450\n",
      "90552\n",
      "90653\n",
      "90753\n",
      "90852\n",
      "90950\n",
      "91047\n",
      "91143\n",
      "91238\n",
      "91332\n",
      "91425\n",
      "91517\n",
      "91608\n",
      "91698\n",
      "91787\n",
      "91875\n",
      "91962\n",
      "92048\n",
      "92133\n",
      "92217\n",
      "92300\n",
      "92382\n",
      "92463\n",
      "92543\n",
      "92622\n",
      "92700\n",
      "92777\n",
      "92853\n",
      "92928\n",
      "93002\n",
      "93075\n",
      "93147\n",
      "93218\n",
      "93288\n",
      "93357\n",
      "93425\n",
      "93492\n",
      "93558\n",
      "93623\n",
      "93687\n",
      "93750\n",
      "93812\n",
      "93873\n",
      "93933\n",
      "93992\n",
      "94050\n",
      "94107\n",
      "94163\n",
      "94218\n",
      "94272\n",
      "94325\n",
      "94377\n",
      "94428\n",
      "94478\n",
      "94527\n",
      "94575\n",
      "94622\n",
      "94668\n",
      "94713\n",
      "94757\n",
      "94800\n",
      "94842\n",
      "94883\n",
      "94923\n",
      "94962\n",
      "95000\n",
      "95037\n",
      "95073\n",
      "95108\n",
      "95142\n",
      "95175\n",
      "95207\n",
      "95238\n",
      "95268\n",
      "95297\n",
      "95325\n",
      "95352\n",
      "95378\n",
      "95403\n",
      "95427\n",
      "95450\n",
      "95472\n",
      "95493\n",
      "95513\n",
      "95532\n",
      "95550\n",
      "95567\n",
      "95583\n",
      "95598\n",
      "95612\n",
      "95625\n",
      "95637\n",
      "95648\n",
      "95658\n",
      "95667\n",
      "95675\n",
      "95682\n",
      "95688\n",
      "95693\n",
      "95697\n",
      "95700\n",
      "95702\n",
      "95703\n",
      "95703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9605127"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_pairs(find_all(expand_columns(expand_rows(real_data)))) #test with new code. Correct haha!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = [6,1]\n",
    "end = [11,5]\n",
    "calc_distance(start, end) # works, my mistake here was not testing it on different cases! I did not account for negative values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 3], [1, 7], [2, 0], [4, 6], [5, 1], [6, 9], [8, 7], [9, 0], [9, 4]]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_all(test_data) # it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test expanding 1o times on test data\n",
    "# function to find all hashtags and stores their locations (returns 2 by x matrix)\n",
    "# function to handle looping through all hashtags (equal to # of pairs)\n",
    "def calc_expanded_distance(hash_1, hash_2, expanded_columns, expanded_rows): # improved functionality\n",
    "    # Calculate horizontal distance\n",
    "    horizontal = abs(hash_2[1] - hash_1[1])\n",
    "    for col in range(min(hash_1[1], hash_2[1]), max(hash_1[1], hash_2[1])):\n",
    "        if col in expanded_columns:\n",
    "            horizontal += 999998\n",
    "\n",
    "    # Calculate vertical distance\n",
    "    vertical = abs(hash_2[0] - hash_1[0])\n",
    "    for row in range(min(hash_1[0], hash_2[0]), max(hash_1[0], hash_2[0])):\n",
    "        if row in expanded_rows:\n",
    "            vertical += 9999998\n",
    "\n",
    "    return horizontal + vertical\n",
    "def find_all(test_data):\n",
    "    locations = []\n",
    "    for i, row in enumerate(test_data):\n",
    "        for j, element in enumerate(row):\n",
    "            if element == '#':\n",
    "                locations.append([i, j])\n",
    "    return locations\n",
    "def handle_pairs(locations, expanded_columns, expanded_rows):\n",
    "    distance = 0\n",
    "    pairs = 0\n",
    "    while locations:\n",
    "        hash = locations.pop(0)  # Remove the first element from locations\n",
    "        for other_hash in locations:\n",
    "            distance += calc_expanded_distance(hash, other_hash, expanded_columns, expanded_rows)\n",
    "            pairs += 1\n",
    "    print(pairs)\n",
    "    return distance\n",
    "# now expanding the universe function:\n",
    "def expand_rows(data):\n",
    "    expanding_rows = data.copy()\n",
    "    insertions = 0\n",
    "    for i in range(len(data)):\n",
    "        if all(element == '.' for element in data[i]):\n",
    "            for times in range(999999):\n",
    "                expanding_rows.insert(i+insertions,data[i])\n",
    "                insertions += 1\n",
    "    return expanding_rows\n",
    "def find_expanded_rows(data): # more efficient than expand_rows()\n",
    "    expanded_rows = []\n",
    "    for i in range(len(data)):\n",
    "        if all(element == '.' for element in data[i]):\n",
    "            expanded_rows.append(i)\n",
    "    return expanded_rows\n",
    "def expand_columns(data): # figured out an easier way! just transpose the data and use expand_rows()!\n",
    "    expanding_columns = data.copy()\n",
    "    insertions = 0\n",
    "    for j in range(len(data[0])):\n",
    "        column = []\n",
    "        column.append([data[x][j] for x in range(len(data))]) # super inefficient.. dataframe would be better\n",
    "        if all(element == '.' for element in column[0]): # used [0] cuz it's a list of list somehow\n",
    "            #expanding_columns.insert() # super inefficient cuz gotta manually insert each element!\n",
    "            for times in range(999999):\n",
    "                for row in range(len(data)):\n",
    "                    expanding_columns[row] = expanding_columns[row][:j+insertions] + column[0][row] + expanding_columns[row][j+insertions:]\n",
    "                insertions += 1\n",
    "    return expanding_columns\n",
    "def find_expanded_columns(data): # more efficient than expand_columns()\n",
    "    expanded_columns = []\n",
    "    for j in range(len(data[0])):\n",
    "        column = [data[i][j] for i in range(len(data))]\n",
    "        if all(element == '.' for element in column):\n",
    "            expanded_columns.append(j)\n",
    "    return expanded_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['...#......',\n",
       " '.......#..',\n",
       " '#.........',\n",
       " '..........',\n",
       " '......#...',\n",
       " '.#........',\n",
       " '.........#',\n",
       " '..........',\n",
       " '.......#..',\n",
       " '#...#.....']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['............#........................',\n",
       " '.........................#...........',\n",
       " '#....................................',\n",
       " '.....................................',\n",
       " '........................#............',\n",
       " '.#...................................',\n",
       " '....................................#',\n",
       " '.....................................',\n",
       " '.........................#...........',\n",
       " '#............#.......................']"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expand_columns(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_pairs(find_all(expand_columns(expand_rows(test_data)))) #correct, now will change it to 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8410"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handle_pairs(find_all(expand_columns(expand_rows(test_data)))) # Correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handle_pairs(find_all(expand_columns(expand_rows(test_data)))) # way too long to run on duplicating 1M times! that's on test data as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gotta change approach completely, instead of actually duplicating dataset, just flag those columns/rows and adjust calculations accordingly!\n",
    "expanded_columns = find_expanded_columns(test_data)\n",
    "expanded_rows = find_expanded_rows(test_data)\n",
    "handle_pairs(find_all(test_data), expanded_columns, expanded_rows) # testing on 10 duplications:\n",
    "# Correct! now apply to 1M:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "406000210"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expanded_columns = find_expanded_columns(test_data)\n",
    "expanded_rows = find_expanded_rows(test_data)\n",
    "handle_pairs(find_all(test_data), expanded_columns, expanded_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3026944230578"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok now on real data:\n",
    "expanded_columns = find_expanded_columns(real_data)\n",
    "expanded_rows = find_expanded_rows(real_data)\n",
    "handle_pairs(find_all(real_data), expanded_columns, expanded_rows)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
